{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will guide you through the process of applying a Gaussian Mixture Model to a set of oceanographic profiles. The code was designed for application to chlorophyll profiles, but places where you might want to make modifications for use with other profiles will be noted. This notebook will also allow you to make sample figures akin to those shown in Echols, Rocap, and Riser (2021). \n",
    "\n",
    "This notebook reflects work done by Rosalind Echols as a PhD candidate at the University of Washington School of Oceanography. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import basic packages\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import random\n",
    "import gsw\n",
    "\n",
    "#import packages for PCA and GMM:\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pandas as pd\n",
    "\n",
    "#import stats packages for analysis\n",
    "from scipy.stats import norm\n",
    "import scipy.stats\n",
    "\n",
    "#import plotting packages; yes, I know Basemap is on the way out\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#set path for saving figures/data if you want:\n",
    "sfpath='/Users/rosalindechols/Documents/Generals/Self_Shading_Research/Analysis/PCA_GMM/FINAL/8pcs_21clusters/'\n",
    "save_plots=False\n",
    "    \n",
    "depth=250\n",
    "max_depth=251\n",
    "\n",
    "#To make subsets for testing out the BIC criterion, you will need to make subsets.  \n",
    "make_tests=True\n",
    "\n",
    "#import QCed, filtered, interpolated file: \n",
    "f='all_chla_argo_250_5m.nc'\n",
    "\n",
    "data=xr.open_dataset(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reported values of chlorophyll from Argo floats are negative; while this is obviously incorrect in the absolute sense, these values may be correct relative to the remainder of the profile. In addition, zero values will also not work for future steps in which we may want to take the logarithm of the data. To address both of these issues, we are going to create a useable version of our data by offsetting all the profiles by the same amount so that all values are positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original maximum and minimum:  43.20624542847094 -0.019999999552965164\n",
      "New maximum and minimum:  43.236245428023906 0.01\n"
     ]
    }
   ],
   "source": [
    "#minimum value is negative so you need to subtract. Could also add the absolute value. \n",
    "\n",
    "#check the current minimum and maximum values of the data\n",
    "print(\"Original maximum and minimum: \", data['CHLA'].values.max(),data['CHLA'].values.min())\n",
    "\n",
    "test_data=data['CHLA'].values-data['CHLA'].values.min()+0.01\n",
    "print(\"New maximum and minimum: \",test_data.max(),test_data.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the type of variable you are working with, the values associated with the profile at each depth may not be normally distributed. This is particularly true in the case of a variable like chlorophyll, where the values at all depths span multiple orders of magnitude, skewed towards low values. We can visualize this by plotting histograms at several different depths, and comparing this with the results after log-normalizing. This is why we need the test data set where we have adjusted so that all values are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do more with the histograms to show that they are skew and can be normalized\n",
    "fig=plt.figure(figsize=(20,8))\n",
    "\n",
    "for n in range(1,11):\n",
    "    ax=fig.add_subplot(2,5,n)\n",
    "    if n<6:\n",
    "        ax.hist(test_data[:,(n-1)*10],bins=np.arange(0,2,0.1),edgecolor='k',facecolor='lightgray')\n",
    "        ax.axvline(np.nanmedian(test_data[:,(n-1)*10]),c='r',linestyle='dashed')\n",
    "        if n==1:\n",
    "            ax.set_ylabel('Number of profiles',fontsize=14)\n",
    "    else:\n",
    "        ax.hist(np.log10(test_data[:,(n-6)*10]),bins=np.arange(-2,1,0.2),edgecolor='k',facecolor='lightgray')\n",
    "        ax.axvline(np.nanmedian(np.log10(test_data[:,(n-6)*10])),c='r',linestyle='dashed')\n",
    "        ax.set_title('%dm' %((n-6)*50),fontsize=14)\n",
    "        if n==6:\n",
    "            ax.set_ylabel('Number of profiles',fontsize=14)\n",
    "        elif n==8:\n",
    "            ax.set_xlabel('log(Chlorophyll (mg m$^{-3}$))',fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.suptitle('Chlorophyll (mg m$^{-3}$)',fontsize=18)\n",
    "\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'concentration_hist.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before developing the Gaussian Mixture Model (GMM), we want to perform principal component analysis. This is ture for several reasons: \n",
    "1) Dimensionality reduction\n",
    "2) Allows us to sort profiles into clusters based on profile shape rather than chlorophyll values\n",
    "\n",
    "For chlorophyll profiles, we will be applying principal component analysis to the log-transformed data. For other variables, log-transformation is not necessary to produce reasonable clusters using GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio, by component:  [0.45247978 0.29244187 0.13169803 0.04863126 0.02260199 0.01314921\n",
      " 0.00807264 0.00549772 0.00385226 0.00287422 0.00220218 0.00173911\n",
      " 0.00141432 0.00116742 0.00099608 0.00084743 0.0007502 ]\n",
      "Cumulative explained variance:  [0.45247978 0.74492165 0.87661967 0.92525093 0.94785292 0.96100214\n",
      " 0.96907477 0.97457249 0.97842475 0.98129897 0.98350115 0.98524026\n",
      " 0.98665458 0.987822   0.98881808 0.98966551 0.99041571]\n"
     ]
    }
   ],
   "source": [
    "#Perform principal component analysis, with a target of 99% of the variance\n",
    "pca=PCA(0.99)\n",
    "pca.fit(np.log(test_data))\n",
    "\n",
    "print(\"Explained variance ratio, by component: \", pca.explained_variance_ratio_)\n",
    "print(\"Cumulative explained variance: \", np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "fig=plt.figure(figsize=(10,6))\n",
    "plt.plot(np.arange(1,pca.n_components_+1),np.cumsum(pca.explained_variance_ratio_),c='lightgray',zorder=1,lw=3)\n",
    "plt.scatter(np.arange(1,pca.n_components_+1),np.cumsum(pca.explained_variance_ratio_),s=50,c='r',zorder=3)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.xlabel('Number of PCs', fontsize=18)\n",
    "plt.ylabel('Explained Variance',fontsize=18)\n",
    "plt.axhline(0.95,linestyle='--',c='teal',label='95%')\n",
    "plt.axhline(0.975,linestyle=':',c='teal',label='97.5%')\n",
    "plt.legend(loc='best',fontsize='x-large')\n",
    "plt.xlim(1,17)\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'PCA_expl_variance.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to visualize the principal components. For the dataset provided here, the first 8 principal components explain ~97.5% of the variance, and includes all the PCs that individually explain >0.5% of the variance. We will plot these and use them for the subsequent clustering; however, it is possible to select a different number of PCs for the later analysis and compare to the results produced by only using 8 PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = cmocean.cm.matter(np.linspace(0,1,8))\n",
    "fig=plt.figure(figsize=(16,8))\n",
    "for i in range(0,8):\n",
    "    fig.add_subplot(2,4,i+1)\n",
    "    plt.plot(pca.components_[i],np.arange(0,251,5),linewidth=3,c=colors[i])\n",
    "    if i==5:\n",
    "        plt.text(0,310,'Chlorophyll (mg m$^{-3}$)',fontsize=18)\n",
    "    if i==4:\n",
    "        plt.text(-0.35,30,'Depth (dbar)',fontsize=18,rotation=90)\n",
    "    plt.ylim(250,0)\n",
    "    plt.tick_params(labelsize=14)\n",
    "\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'PCA_comps.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Mixture Models require pre-selection of a number of clusters. In order to select a number of clusters, we need to use a method that evaluates the tradeoffs between number of clusters and quality of fit. Both the Bayesian Information Criterion (BIC) and Akaike Information Criteria (AIC) accomplish this, although BIC has a harsher penalty as the number of clusters increases. In order to investigate the optimal number of clusters, we will first subset the data, selecting one profile from each 1x1 degree box in which data has been gathered. Using 20 different randomly selected test-sets following this approach, we can estimate the trend in BIC as the number of clusters increases as well as the uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Latitude:  -70\n",
      "Current Latitude:  -60\n",
      "Current Latitude:  -50\n",
      "Current Latitude:  -40\n",
      "Current Latitude:  -30\n",
      "Current Latitude:  -20\n",
      "Current Latitude:  -10\n",
      "Current Latitude:  0\n",
      "Current Latitude:  10\n",
      "Current Latitude:  20\n",
      "Current Latitude:  30\n",
      "Current Latitude:  40\n",
      "Current Latitude:  50\n",
      "Current Latitude:  60\n",
      "Current Latitude:  70\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#make subsets for future use in testing out the BIC criterion for varying numbers of clusters\n",
    "if make_tests==True:\n",
    "    #Initialize array of nans; will remove unused data later\n",
    "    tlist=np.nan*np.ones((20,len(data['LATITUDE'])//5))\n",
    "    \n",
    "    #sort data by latitude\n",
    "    lat_sort=sorted(data['LATITUDE'].values)\n",
    "    \n",
    "    #retain original indices of sorted data\n",
    "    lat_sort_ind=data['LATITUDE'].values.argsort()\n",
    "    \n",
    "    start=0\n",
    "    count=0\n",
    "    for n in range(-76, 78):\n",
    "        if n%10==0:\n",
    "            print(\"Current Latitude: \",n)\n",
    "        #find 1-degree latitude subset\n",
    "        end=next(i for i, j in enumerate(lat_sort) if j>=n+1)\n",
    "        for nn in np.arange(-180,180.1):\n",
    "            lon_set=[i for i,j in enumerate(data['LONGITUDE'].values[lat_sort_ind[start:end]]) if nn <= j < nn+1]\n",
    "            if len(lon_set)==0:\n",
    "                pass\n",
    "            else:\n",
    "                for nnn in range(0,20):\n",
    "                     #randomly select 20 times to create test subsets\n",
    "                    #for 1x1 boxes with a small number of profiles, this won't do much\n",
    "                    tlist[nnn,count]=random.choice(lat_sort_ind[start:end][lon_set])\n",
    "                count+=1\n",
    "        start=end\n",
    "\n",
    "    #only retain the values in the array that are actually used\n",
    "    tlist=np.array(tlist[:,0:count],dtype='int')\n",
    "    np.savetxt(sfpath+\"1x1_test_lists.csv\", tlist, delimiter=\",\")\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the locations of one of these subsets just to make sure the code is doing what we think it is doing. We can do this in two ways: first, check to make sure that the profiles are distributed across all latitudes. At the same time, we can see how the distribution of the test profiles compares to the overall distribution of profiles. Latitudes that are better represented in the test set than in the complete data set have data across more longitudes (i.e. the Southern Ocean) than in other areas (i.e. the Mediterranean Sea). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(16,8))\n",
    "ax1=fig.add_subplot(1,2,1)\n",
    "ax1.hist(data['LATITUDE'].values,bins=np.arange(-75,76,10),edgecolor='m',linewidth=2,facecolor='gray',label='All Profiles',alpha=0.5,density=1)\n",
    "ax1.set_ylabel('Fraction of Profiles',fontsize=18)\n",
    "ax1.set_ylim(0,0.025)\n",
    "plt.tick_params(labelsize=16)\n",
    "ax1.set_xlabel('Latitude',fontsize=18)\n",
    "plt.legend(loc='best',fontsize='x-large')\n",
    "ax2=fig.add_subplot(1,2,2)\n",
    "ax2.hist(data['LATITUDE'].values[tlist[0]],bins=np.arange(-75,76,10),edgecolor='m',linewidth=2,facecolor='gray',label='Test Set',alpha=0.5,density=1)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.legend(loc='best',fontsize='x-large')\n",
    "ax2.set_xlabel('Latitude',fontsize=18)\n",
    "ax2.set_ylim(0,0.025)\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'latitude_hist.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second method is tos simply plot the points on a map. If it's working, we should see data points all over the globe (excepting areas where there is no data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12,10))\n",
    "m = Basemap(projection='robin',lon_0=-180,resolution='c')\n",
    "m.drawcoastlines()\n",
    "m.fillcontinents(color='gray')\n",
    "cp=m.scatter(data['LONGITUDE'].values[tlist[0]],data['LATITUDE'].values[tlist[0]],latlon='True',s=5)\n",
    "m.drawparallels(np.arange(-90., 81., 30.), labels = [1,0,0,0], fontsize = 16)\n",
    "m.drawmeridians(np.arange(-180., 181., 90.), labels = [0,0,1,0], fontsize = 16)\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'map_test_set.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now transform our data into PC space, and test out a range of clustering options for each of our subsets. This step takes a long time because the GMM algorithm is computationally expensive. The more principal components you decide to retain, the longer this will take. You can also test it out with different number of PCS and compare the outcome. Just be prepared to run the cell and walk away for a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently testing subset 1.\n",
      "Currently testing subset 6.\n",
      "Currently testing subset 11.\n",
      "Currently testing subset 16.\n"
     ]
    }
   ],
   "source": [
    "#Define empty arrays to store AIC and BIC values; we will only end up using the BIC values, but it is \n",
    "#instructional to see\n",
    "\n",
    "#do you want to save the criterion data?\n",
    "save_BIC=True\n",
    "#select the number of principal components to use:\n",
    "pcs=17\n",
    "#set the maximum number of clusters you want to test:\n",
    "max_clusters=50\n",
    "\n",
    "#transform profiles into PC space:\n",
    "pca=PCA(n_components=pcs)\n",
    "pca.fit(np.log(test_data))\n",
    "training=pca.transform(np.log(test_data))\n",
    "\n",
    "BIC=np.zeros((len(tlist),max_clusters))\n",
    "AIC=np.zeros((len(tlist),max_clusters))\n",
    "for i in range(0,len(tlist)):\n",
    "    if i%5==0:\n",
    "        print(\"Currently testing subset %d.\" %(i+1))\n",
    "    n_components=np.arange(1, 51)\n",
    "    subset=training[tlist[i]]\n",
    "    models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(subset) for n in n_components]\n",
    "    BIC[i]=[m.bic(subset) for m in models]\n",
    "    AIC[i]=[m.aic(subset) for m in models]\n",
    "    \n",
    "if save_BIC==True:\n",
    "    np.savetxt(sfpath+\"pca%d_BIC_results.txt\" %pcs, BIC, delimiter=\",\")\n",
    "    np.savetxt(sfpath+\"pca%d_AIC_results.txt\" %pcs, AIC, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the BIC (and AIC if we want) to see the range of cluster numbers that are ideal for this problem. It is unlikely that a single \"best\" number of clusters will emerge from using BIC, which means that we will need to \"use our judgement\" at some point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum BIC value = 75537 at 20 clusters\n"
     ]
    }
   ],
   "source": [
    "plot_AIC=False\n",
    "\n",
    "#plot BIC\n",
    "min_BIC=next([i+1,j] for i,j in enumerate(np.mean(BIC,axis=0)) if j==min(np.mean(BIC,axis=0)))\n",
    "print(\"Minimum BIC value = %1.0f at %d clusters\" %(min_BIC[1],min_BIC[0]))\n",
    "fig=plt.figure(figsize=(10,8))\n",
    "plt.errorbar(n_components,np.mean(BIC,axis=0),yerr=np.std(BIC,axis=0),c='b',label='BIC',lw=3)\n",
    "\n",
    "#plot AIC\n",
    "if plot_AIC==True:\n",
    "    plt.errorbar(n_components,np.mean(AIC,axis=0),yerr=np.std(BIC,axis=0),c='r',label='AIC',lw=3)\n",
    "\n",
    "plt.legend(loc='best',fontsize='x-large')\n",
    "plt.xlabel('Number of Clusters',fontsize=18)\n",
    "plt.ylabel('BIC score',fontsize=18)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.ylim(min(np.mean(BIC,axis=0))-0.1*min(np.mean(BIC,axis=0)),np.mean(BIC,axis=0)[5])\n",
    "plt.xlim(5,35)\n",
    "\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'BIC_plot.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that (for 17 PCs) there is a wide approximate minimum between 17 and 23 clusters. For the remaining sections of this notebook, we will use 21 clusters, for reasons that will be discussed in relation to some of the figures. However, all of the sections can be run with a different number of clusters for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of clusters to use taken from decision based on BIC plot\n",
    "gmm_comps=21\n",
    "\n",
    "#redoing the PCA here in case you were experimenting with different #s of PCs in previous cells\n",
    "pca=PCA(0.99)\n",
    "pca.fit(np.log(test_data))\n",
    "training=pca.transform(np.log(test_data))\n",
    "\n",
    "gmm = GaussianMixture(n_components=gmm_comps)\n",
    "#this is where you train\n",
    "gmm.fit(training)\n",
    "probs = gmm.predict_proba(training)\n",
    "#the labels are the groups; there is no de facto order to the clusters, so you will get different\n",
    "#cluster numbers each time you run this, even if a particular profile is always assigned to the same\n",
    "#cluster; we will sort the groups later\n",
    "labels = gmm.predict(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful first plot to make is a histogram of how many profiles are in each group. Although the BIC criterion in theory allowed us to select a quasi-optimal number of clusters, it is still helpful to know what this looks like in practice. Are some of the clusters really small? Is there one cluster that has a disproportionately large number of profiles assigned to it? This is a very superficial look, but starts helping us understand the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,8))\n",
    "plt.hist(labels+1, bins=np.arange(1,gmm_comps+2),edgecolor='k',linewidth=2,facecolor='lightgray')\n",
    "plt.xlabel('Cluster Number', fontsize=18)\n",
    "plt.ylabel('Number of Profiles',fontsize=18)\n",
    "plt.tick_params(labelsize=16)\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'GMM_cluster_hist.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to start looking at the characteristics of each cluster. Because we did PCA, looking at the PC statistics may not be super informative. Instead, we can start looking at average profile shape, probability of assignment to each cluster, geographical distribution, and so on, all derived from the raw profiles assigned to each group.  \n",
    "\n",
    "We'll start by looking at general profile characteristics for each group. We can do this in a variety of ways: an average profile; a weighted average profile (weighted by the probability that an individual profile is assigned to a particular group); and a median profile. Because GMM assigns a probability that each item is in a particular group, the weighted average gives more weight to the profiles that have the highest probability of being in a particular group. Profiles that may be bordering between two groups and thus have a lower probability of being in either group will have less weight in constructing that profile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find averages\n"
     ]
    }
   ],
   "source": [
    "print('Find averages')\n",
    "#regular average\n",
    "group_ave={}\n",
    "#weighted average\n",
    "group_wave={}\n",
    "#median\n",
    "group_median={}\n",
    "#standard deviation\n",
    "group_std={}\n",
    "#weighted standard deviation\n",
    "group_wstd={}\n",
    "for var in ['CHLA']:\n",
    "    group_ave[var]=np.nan*np.ones((gmm_comps,51))\n",
    "    group_wave[var]=np.nan*np.ones((gmm_comps,51))\n",
    "    group_median[var]=np.nan*np.ones((gmm_comps,51))\n",
    "    group_std[var]=np.nan*np.ones((gmm_comps,51))\n",
    "    group_wstd[var]=np.nan*np.ones((gmm_comps,51))\n",
    "\n",
    "for var in ['CHLA']:\n",
    "    for m in range(gmm_comps):\n",
    "        d=[i for i,j in enumerate(labels) if j==m]\n",
    "        #regular average\n",
    "        group_ave[var][m]=np.nanmean(data[var][d],0)\n",
    "        #weighted average\n",
    "        group_wave[var][m]=np.average(data[var][d],0,weights=probs[d,m])\n",
    "        #standard deviation\n",
    "        group_std[var][m]=np.nanstd(data[var][d],0)\n",
    "        #weighted standard deviation\n",
    "        group_median[var][m]=np.nanmedian(data[var][d],0)\n",
    "        variance = np.average((data[var][d]-group_ave[var][m])**2,0, weights=probs[d,m])\n",
    "        group_wstd[var][m]=variance**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labeling of groups coming out of GMM is random, and thus group 0 for any given run does not represent anything particularly special (and would likely be different from group 0 if you ran the exact same code a second time). One way we can get around this is by sorting the average profiles for each group based on some characteristic of that average profile, like the surface chlorophyll. For the purposes of exploring the data, we'll do that here, but there are many other options for how it might make sense to sequence the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 0, 10, 3, 7, 18, 13, 2, 19, 6, 20, 16, 15, 8, 12, 1, 9, 17, 4, 14, 11]\n"
     ]
    }
   ],
   "source": [
    "def sort_chl_profs(data,labels,groups=12,keyword='surf'):\n",
    "    #resequence CHL groups\n",
    "    #find surface chl\n",
    "    new_chl=np.zeros(groups)\n",
    "    #print(groups,len(data['CHLA']))\n",
    "    count=0\n",
    "    for prof in data['CHLA']:\n",
    "        if keyword=='surf':\n",
    "            new_chl[count]=prof[0]\n",
    "        else:\n",
    "            new_chl[count]=np.trapz(prof)\n",
    "        count+=1\n",
    "    \n",
    "    new_groups=sorted(range(len(new_chl)), key=lambda k: new_chl[k])\n",
    "    \n",
    "    new_labels=np.zeros(len(labels))\n",
    "    for n in range(0,len(labels)):\n",
    "        new_labels[n]=next(i for i, j in enumerate(new_groups) if j==labels[n])  \n",
    "    \n",
    "    return new_groups,new_labels\n",
    "\n",
    "#select a sorting group (group_ave, group_wave, or group_median) for sorting by the surface chlorophyll\n",
    "#associated with each group average.\n",
    "sorting_group=group_wave\n",
    "sorting_std=group_wstd\n",
    "\n",
    "new_groups,new_labels=sort_chl_profs(sorting_group,labels,groups=gmm_comps,keyword='surf')\n",
    "\n",
    "print(new_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start visualizing each of clusters and exploring the statistics of the GMM output. First up, we'll look at individual clusters. The clusters are now sequenced based the surface chlorophyll of whichever central tendency statistic you selected in the previous cell. First, we'll just plot all the profiles. This will be busy but will give us a quick snapshot of the overal cluster shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(21,9))\n",
    "color = cmocean.cm.matter(np.linspace(0,1,gmm_comps))\n",
    "for n in range(gmm_comps):\n",
    "    ax=fig.add_subplot(3,7,n+1)\n",
    "    ax.plot(sorting_group['CHLA'][new_groups[n]],data['DEPTH'][0],lw=3,c=color[n],zorder=3)\n",
    "    if n==7:\n",
    "        ax.set_ylabel('Depth',fontsize=18)\n",
    "    if n==17:\n",
    "        ax.set_xlabel('Chlorophyll (mg m$^{-3}$)',fontsize=18)\n",
    "    ax.tick_params(labelsize=14)\n",
    "    ax.set_ylim(max_depth,0)\n",
    "    #set all the x-limits to the same for better comparison (even though\n",
    "    #some clusters will have much higher concentrations)\n",
    "    ax.set_xlim(-0.01,1.3)\n",
    "    \n",
    "plt.tight_layout()\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'profiles/GMM_all_profs.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, for each of these clusters, we can look a little more carefully at what's going on in each cluster. Lets look at several features to start:\n",
    "1. The central tendency profile (average, weighted average, or median) with a subset of individual profiles\n",
    "2. The probability distribution for that cluster (i.e. what is the distribution of probabilities for profiles assigned to that group)\n",
    "3. The difference between the average, weighted average, and median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select groups to plot below\n",
    "plot_group=np.arange(21)\n",
    "\n",
    "color = cmocean.cm.matter(np.linspace(0,1,gmm_comps))\n",
    "\n",
    "for nnn in plot_group:\n",
    "    #plot average profile and representative subset\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 10])\n",
    "    ax1.plot(sorting_group['CHLA'][new_groups[nnn]],data['DEPTH'][0],lw=3,c=color[nnn],zorder=3)\n",
    "    ax1.plot(sorting_group['CHLA'][new_groups[nnn]]+sorting_std['CHLA'][new_groups[nnn]],data['DEPTH'][0],lw=2,linestyle='--',c=color[nnn],zorder=3)\n",
    "    ax1.plot(sorting_group['CHLA'][new_groups[nnn]]-sorting_std['CHLA'][new_groups[nnn]],data['DEPTH'][0],lw=2,linestyle='--',c=color[nnn],zorder=3)\n",
    "    d=[i for i,j in enumerate(labels) if j==new_groups[nnn]]\n",
    "    for m in range(0,len(d),50):\n",
    "        ax1.plot(data['CHLA'][d][m],data['DEPTH'][0],lw=0.5,alpha=0.5,color='lightgray',zorder=1)\n",
    "\n",
    "    #plt.legend(loc='best',fontsize='large')\n",
    "    ax1.set_ylim(max_depth,0)\n",
    "    ax1.set_xlim(0-sorting_std['CHLA'][new_groups[nnn]].max(),sorting_group['CHLA'][new_groups[nnn]].max()+2*sorting_std['CHLA'][new_groups[nnn]].max())\n",
    "    ax1.set_ylabel('Depth (m)',fontsize=18)\n",
    "    ax1.set_xlabel('Chlorophyll (mg m$^{-3}$)',fontsize=18)\n",
    "    ax1.tick_params(labelsize=16)\n",
    "    \n",
    "    axins = inset_axes(ax1, width=3, height=2,loc=4,borderpad=2)\n",
    "    axins.hist(probs[d,new_groups[nnn]],bins=np.arange(0,1.05,0.1),edgecolor='k',facecolor='lightgray')\n",
    "    axins.axhline(0.75*len(d),color='red',linestyle='dotted',lw=0.5)\n",
    "    axins.axhline(0.9*len(d),color='red',linestyle='dashed',lw=0.5)\n",
    "    axins.set_ylabel('# of Profiles')\n",
    "    axins.set_xlabel('Cluster probability')\n",
    "    axins.xaxis.tick_top()\n",
    "    axins.xaxis.set_label_position('top') \n",
    "    \n",
    "    #plot each central tendency profile for comparison\n",
    "    ax2.plot(group_ave['CHLA'][new_groups[nnn]],data['DEPTH'][0],lw=3,c=color[2],zorder=3,label='Average')\n",
    "    ax2.plot(group_wave['CHLA'][new_groups[nnn]],data['DEPTH'][0],lw=3,c=color[10],zorder=2,label='Weighted Average')\n",
    "    ax2.plot(group_median['CHLA'][new_groups[nnn]],data['DEPTH'][0],lw=3,c=color[18],zorder=1,label='Median')\n",
    "    ax2.set_ylim(max_depth,0)\n",
    "    ax2.set_xlim(0-sorting_std['CHLA'][new_groups[nnn]].max(),sorting_group['CHLA'][new_groups[nnn]].max()+sorting_std['CHLA'][new_groups[nnn]].max())\n",
    "    ax2.set_xlabel('Chlorophyll (mg m$^{-3}$)',fontsize=18)\n",
    "    ax2.tick_params(labelsize=16)\n",
    "    ax2.legend(loc='best',fontsize='x-large')\n",
    "    \n",
    "    if save_plots==True:\n",
    "        plt.savefig(sfpath+'profiles/GMM_mean_prof_cluster%d.pdf' %nnn,format='pdf',bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful thing to look at to start understanding the clusters is to look at where they occur. As with looking at the profiles, we can select a cluster number to look at and run the code in the following cell, which will produce a map showing where those profiles occurred (and when)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find months\n",
      "Find seasons\n"
     ]
    }
   ],
   "source": [
    "#first we'll determine the months and seasons for each profile. To be able to look at N/S similarity,\n",
    "#we'll look at everything in terms of equivalent northern month.\n",
    "\n",
    "dates = pd.to_datetime(data['JULD'].values).month\n",
    "print('Find months')\n",
    "months=np.zeros(len(dates))\n",
    "for m in range(0,len(months)):\n",
    "    if data['LATITUDE'].values[m]>=0:\n",
    "        months[m]=dates[m]\n",
    "    else:\n",
    "        if dates[m]>=7:\n",
    "            months[m]=dates[m]-6\n",
    "        else:\n",
    "            months[m]=dates[m]+6\n",
    "print('Find seasons')\n",
    "seasons_north=[[12,1,2],[3,4,5],[6,7,8],[9,10,11]]\n",
    "seasons=np.zeros(len(months))\n",
    "for n in range(0,len(months)):\n",
    "    for nn in range(0,len(seasons_north)):\n",
    "        if months[n] in seasons_north[nn]:\n",
    "            seasons[n]=nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group=np.arange(21)\n",
    "\n",
    "for nnn in plot_group:\n",
    "    subset=[i for i,j in enumerate(labels) if j==new_groups[nnn]]\n",
    "\n",
    "    cmap = cmocean.cm.haline  # define the colormap\n",
    "    # extract all colors from the color map\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "\n",
    "    # create the new segmented (vs. continuous) map\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "        'Custom cmap', cmaplist, cmap.N)\n",
    "    bounds = np.linspace(-0.5, 3.5, num=5)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    fig=plt.figure(figsize=(14,7))\n",
    "    m = Basemap(projection='robin',lon_0=-0,resolution='c')\n",
    "    cp=m.scatter(data['LONGITUDE'].values[subset],data['LATITUDE'].values[subset],c=seasons[subset],s=5,cmap=cmap, norm=norm,latlon='True')\n",
    "    cp.set_rasterized(True)\n",
    "    m.drawcoastlines()\n",
    "    m.fillcontinents(color='gray')\n",
    "    m.drawparallels(np.arange(-60., 61., 30.), labels = [1,0,0,0], fontsize = 16)\n",
    "    m.drawmeridians(np.arange(-180., 181., 60.), labels = [0,0,0,1], fontsize = 16)\n",
    "    cbar=fig.colorbar(cp,ticks=[0,1,2,3],shrink=0.75)\n",
    "    cbar.ax.set_yticklabels(['Winter','Spring','Summer','Fall'],fontsize=14)\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "    if save_plots==True:\n",
    "        plt.savefig(sfpath+'maps/GMM_map_cluster_%d.pdf' %nnn,format='pdf',bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not always clear from the map plots what the actual distribution across seasons looks like for each cluster, so we can get a better view of this with some histograms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histograms of \"months\" and \"seasons\"-->everything is normalized to northern months to make N/S comparison easier. \n",
    "\n",
    "fig=plt.figure(figsize=(22,9))\n",
    "for n in range(gmm_comps):\n",
    "    d=[i for i,j in enumerate(labels) if j==new_groups[n]]\n",
    "    ax=fig.add_subplot(3,7,n+1)\n",
    "    ax.hist(seasons[d],bins=np.arange(0,5),edgecolor='k',facecolor='lightgray',linewidth=2)\n",
    "    ax.tick_params(labelsize=14)\n",
    "    if n==7:\n",
    "        ax.set_ylabel('Number of Profiles',fontsize=18)\n",
    "    if n==17:\n",
    "        ax.set_xlabel('Season',fontsize=18)\n",
    "    if n<=13:\n",
    "        frame1 = plt.gca()\n",
    "        frame1.axes.get_xaxis().set_ticks([0.5,1.5,2.5,3.5])\n",
    "        frame1.axes.get_xaxis().set_ticklabels([])\n",
    "    elif n>13:\n",
    "        frame1 = plt.gca()\n",
    "        frame1.axes.get_xaxis().set_ticks([0.5,1.5,2.5,3.5])\n",
    "        frame1.axes.get_xaxis().set_ticklabels(['Winter','Spring','Summer','Fall'],rotation=45)\n",
    "        \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'GMM_seasons_hist.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at some statistics in other ways. To make this as useful as possible, we will need to do some manual sorting. For example, based on the plot of the all of the average profiles above, it looks like we have three broad categories of profiles: pure SCM (low surface Chl, peak at depth); well-mixed/sigmoid (looks a lot like a typical mixed layer); and combination (higher surface Chl, but still with a peak at depth). For the 21 clusters we're working with, these can be broken down as follows (if you decided to tinker, you'll need to do this part manually in the next cell):\n",
    "scm_group = [0,1,2,3,5]\n",
    "well_mixed = [4,6,7,9,10,11,12,17]\n",
    "combo = [8,13,14,15,16,18,19,20]\n",
    "\n",
    "Some of these are a bit of a judgment call and there may not be a rigid delineation in all cases. For the next bit of analysis, we just need to get close in order for it to be useful. First, we'll plot the average profiles for each of the sub-groups on the same axes, which will allow us to see the range of profile shapes across a particular cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scm_group = [0,1,2,3,5]\n",
    "well_mixed = [4,6,7,9,10,11,12,17]\n",
    "combo = [8,13,14,15,16,18,19,20]\n",
    "\n",
    "fig=plt.figure(figsize=(18,8))\n",
    "count=1\n",
    "for g1 in ['scm_group','well_mixed','combo']:\n",
    "    if g1=='scm_group':\n",
    "        # test=group1\n",
    "        g=scm_group\n",
    "    elif g1=='well_mixed':\n",
    "        # test=group2\n",
    "        g=well_mixed\n",
    "    else:\n",
    "        # test=group3\n",
    "        g=combo\n",
    "    \n",
    "    color = cmocean.cm.matter(np.linspace(0,1,len(g)))\n",
    "    ax1=fig.add_subplot(1,3,count)\n",
    "    nn=0\n",
    "    for n in g:\n",
    "        plt.plot(group_wave['CHLA'][new_groups[n]],data['DEPTH'][0],lw=4,c=color[nn])\n",
    "        nn+=1\n",
    "    plt.ylim(250,0)\n",
    "    plt.xlabel('Chlorophyll (mg m$^{-3}$)',fontsize=18)\n",
    "    plt.ylabel('Depth (m)', fontsize=18)\n",
    "    plt.tick_params(labelsize=16)\n",
    "    # plt.legend(loc=4,fontsize='x-large')\n",
    "    count+=1\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'all_profs_sorted.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to make a \"heatmap\" that allows us to look at the two highest probabilities from the GMM output. This way we can see how the highest probability and second highest probability relate in terms of profile shape. If the algorithm is working well, we would expect the second highest probability cluster to share many of the same characteristics as the highst probability group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a heatmap\n",
    "#sort the probabilities for each profile and retain the location of the \n",
    "#two highest probabilities; from a previous cell, the probability output\n",
    "#has the variable name \"probs\"\n",
    "comps=[np.argsort(probs)[-1:-3:-1] for probs in probs]\n",
    "#select the two highest probabilities\n",
    "prob=[probs[n][comps[n]] for n in range(0, len(probs))]\n",
    "\n",
    "#sort groups so that similar shapes are near each other\n",
    "new_seq=[0,1,2,3,5,4,6,7,9,10,11,12,17,8,13,14,15,16,18,19,20]\n",
    "ax_lab=[1,2,3,4,6,5,7,8,10,11,12,13,18,9,14,15,16,17,19,20,21]\n",
    "\n",
    "locs={}\n",
    "for n in range(0,21):\n",
    "    #take sorted list and organize by profile shape\n",
    "    locs[str(new_groups[n])]=next(i for i,j in enumerate(new_seq) if j==n)\n",
    "\n",
    "pair_array=np.zeros((21,21))\n",
    "mags=np.zeros(21)\n",
    "\n",
    "count=0\n",
    "for c in comps:\n",
    "#     # print(locs[str(c[0])],locs[str(c[1])])\n",
    "#     # pair_array[locs[str(c[0])],locs[str(c[1])]]+=1\n",
    "    pair_array[locs[str(c[0])],locs[str(c[1])]]+=prob[count][1]\n",
    "    pair_array[locs[str(c[0])],locs[str(c[0])]]+=prob[count][0]\n",
    "    mags[locs[str(c[0])]]+=1\n",
    "    count+=1\n",
    "\n",
    "# # for n in range(0,21):\n",
    "#     # pair_array[n,n]=np.nan\n",
    "from matplotlib import colors\n",
    "fig=plt.figure(figsize=(12,10))\n",
    "\n",
    "cp=plt.pcolor(pair_array.transpose()/mags,cmap=cmocean.cm.amp,vmin=1e-2,vmax=1,norm=colors.LogNorm())\n",
    "plt.tick_params(labelsize=16)\n",
    "#plt.grid()\n",
    "#plt.xticks(ticks=[0,3,6,9,12,15,18],labels=np.array(new_seq)[0,3,6,9,12,15,18]+1)\n",
    "#plt.yticks(ticks=[0,3,6,9,12,15,18],labels=np.array(new_seq)[0,3,6,9,12,15,18]+1)\n",
    "plt.xticks(ticks=np.arange(0.5,21.5,1),labels=ax_lab)\n",
    "plt.yticks(ticks=np.arange(0.5,21.5,1),labels=ax_lab)\n",
    "plt.axvline(5,linestyle='dashed',color='k',linewidth=2)\n",
    "plt.axvline(13,linestyle='dashed',color='k',linewidth=2)\n",
    "plt.axhline(5,linestyle='dotted',color='k',linewidth=2)\n",
    "plt.axhline(13,linestyle='dotted',color='k',linewidth=2)\n",
    "plt.xlabel('Highest Probability Group',fontsize=20)\n",
    "plt.ylabel('Second Highest Probability Group',fontsize=20)\n",
    "cbar=fig.colorbar(cp)\n",
    "cbar.set_label('Relative frequency',fontsize=18)\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'GMM_probs_heatmap.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view all of the data spatially and temporally in a simplified manner, we can make a modified Hovmuller plot. This is useful for gathering a general impression of the data, but should really just be a precursor to further analysis given that the whole globe is plotted in a 1D slide for each time point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cmocean.cm.matter  # define the colormap\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "\n",
    "# create the new map\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    'Custom cmap', cmaplist, cmap.N)\n",
    "bounds = np.linspace(0, 22, num=22)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig=plt.figure(figsize=(14,6))\n",
    "cp=plt.scatter(data['LATITUDE'],months,c=new_labels+1,marker='|',s=700,alpha=0.9,cmap=cmap, norm=norm)\n",
    "plt.tick_params(labelsize=14)\n",
    "#plt.ylabel('Season',fontsize=16)\n",
    "plt.xlim(-76,79)\n",
    "plt.xlabel('Latitude',fontsize=16)\n",
    "frame1 = plt.gca()\n",
    "frame1.axes.get_yaxis().set_ticks([1,4,7,10])\n",
    "frame1.axes.get_yaxis().set_ticklabels(['Winter','Spring','Summer','Fall'])\n",
    "cbar=fig.colorbar(cp,ticks=[2,4,6,8,10,12,14,16,18,20])\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "# cbar.ax.set_xticklabels()\n",
    "cbar.set_label('Group',fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'GMM_hov.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot might be more useful viewed in a couple alternative setups: first, separated out by basin; and second, separated out across the three broad groups of profile shapes. This will help us understand which of the broad patterns shown above are global in nature in which are specific to a certain basin. For the basin separation, we will also separate out the Mediterranean and Black Seas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Mediterranean/Black Sea profiles: 9775\n",
      "Number of Indian Ocean profiles: 20829\n",
      "Number of Atlantic Ocean profiles: 21348\n",
      "Number of Pacific Ocean profiles: 19320\n"
     ]
    }
   ],
   "source": [
    "#define subgroups for each basin\n",
    "med_black=[]\n",
    "indian=[]\n",
    "atlantic=[]\n",
    "pacific=[]\n",
    "\n",
    "#sort each profile into the appropriate bin\n",
    "#these are sequenced from easiest to separate to most difficult to separate\n",
    "for n in range(0,len(data['LATITUDE'])):\n",
    "    if 0<data['LONGITUDE'].values[n]<42.5 and 30<data['LATITUDE'].values[n]<47:\n",
    "        med_black.append(n)\n",
    "    elif 30<data['LONGITUDE'].values[n]<120 and -80<data['LATITUDE'].values[n]<30:\n",
    "        indian.append(n)\n",
    "    elif -70<data['LONGITUDE'].values[n]<30 and -80<data['LATITUDE'].values[n]<=0:\n",
    "        atlantic.append(n)\n",
    "    elif -80<data['LONGITUDE'].values[n]<30 and 0<data['LATITUDE'].values[n]<80:\n",
    "        atlantic.append(n)\n",
    "    else:\n",
    "        pacific.append(n)\n",
    "    \n",
    "print('Number of Mediterranean/Black Sea profiles: %d' %len(med_black))\n",
    "print('Number of Indian Ocean profiles: %d' %len(indian))\n",
    "print('Number of Atlantic Ocean profiles: %d' %len(atlantic))\n",
    "print('Number of Pacific Ocean profiles: %d' %len(pacific))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can repeat the HOV plotting, but with the basins separated out into subplots\n",
    "cmap = cmocean.cm.matter  # define the colormap\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "\n",
    "regions=[indian,atlantic,pacific]\n",
    "titles=['Indian Ocean','Atlantic Ocean','Pacific Ocean']\n",
    "\n",
    "# create the new map\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    'Custom cmap', cmaplist, cmap.N)\n",
    "bounds = np.linspace(0, 22, num=22)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig=plt.figure(figsize=(12,10))\n",
    "for n in range(0,3):\n",
    "    ax=fig.add_subplot(3,1,n+1)\n",
    "    cp=ax.scatter(data['LATITUDE'][regions[n]],months[regions[n]],c=new_labels[regions[n]]+1,marker='|',s=700,alpha=0.9,cmap=cmap, norm=norm)\n",
    "    ax.tick_params(labelsize=14)\n",
    "    ax.set_xlim(-76,79)\n",
    "    ax.set_title(titles[n],fontsize=16)\n",
    "    if n==2:\n",
    "        ax.set_xlabel('Latitude',fontsize=16)\n",
    "        frame1 = plt.gca()\n",
    "        frame1.axes.get_yaxis().set_ticks([1,4,7,10])\n",
    "        frame1.axes.get_yaxis().set_ticklabels(['Winter','Spring','Summer','Fall'])\n",
    "    else:\n",
    "        frame1 = plt.gca()\n",
    "        frame1.axes.xaxis.set_ticklabels([])\n",
    "        frame1.axes.get_yaxis().set_ticks([1,4,7,10])\n",
    "        frame1.axes.get_yaxis().set_ticklabels(['Winter','Spring','Summer','Fall'])\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.95, 0.25, 0.03, 0.5])\n",
    "cbar = fig.colorbar(cp,cax=cbar_ax,orientation='vertical',ticks=[2,4,6,8,10,12,14,16,18,20])\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "# cbar.ax.set_xticklabels()\n",
    "cbar.set_label('Group',fontsize=14)\n",
    "\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'GMM_hov_basin.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total profiles in SCM group: 16631\n",
      "Total profiles in Well-Mixed group: 24828\n",
      "Total profiles in Combo group: 29813\n"
     ]
    }
   ],
   "source": [
    "#this definition of the subgroups is copied from above. As stated earlier, these groups were selected\n",
    "#manually based on the original data, PCA, and GMM paramaters. If you depart significantly from those,\n",
    "#you may need to change these groups.\n",
    "scm_group = [0,1,2,3,5]\n",
    "well_mixed = [4,6,7,9,10,11,12,17]\n",
    "combo = [8,13,14,15,16,18,19,20]\n",
    "\n",
    "scm_subset=[i for i,j in enumerate(labels) if j in np.array(new_groups)[scm_group]]\n",
    "well_mixed_subset=[i for i,j in enumerate(labels) if j in np.array(new_groups)[well_mixed]]\n",
    "combo_subset=[i for i,j in enumerate(labels) if j in np.array(new_groups)[combo]]\n",
    "\n",
    "print('Total profiles in SCM group: %d' %len(scm_subset))\n",
    "print('Total profiles in Well-Mixed group: %d' %len(well_mixed_subset))\n",
    "print('Total profiles in Combo group: %d' %len(combo_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can repeat the HOV plotting again, but with each subgroup separated into subplots\n",
    "regions=[scm_subset,well_mixed_subset,combo_subset]\n",
    "titles=['SCM','Well-Mixed','Combo']\n",
    "\n",
    "# create the new map\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    'Custom cmap', cmaplist, cmap.N)\n",
    "bounds = np.linspace(0, 22, num=22)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig=plt.figure(figsize=(12,10))\n",
    "for n in range(0,3):\n",
    "    ax=fig.add_subplot(3,1,n+1)\n",
    "    cp=ax.scatter(data['LATITUDE'][regions[n]],months[regions[n]],c=new_labels[regions[n]]+1,marker='|',s=700,alpha=0.9,cmap=cmap, norm=norm)\n",
    "    ax.tick_params(labelsize=14)\n",
    "    ax.set_xlim(-76,79)\n",
    "    ax.set_title(titles[n],fontsize=16)\n",
    "    if n==2:\n",
    "        ax.set_xlabel('Latitude',fontsize=16)\n",
    "        frame1 = plt.gca()\n",
    "        frame1.axes.get_yaxis().set_ticks([1,4,7,10])\n",
    "        frame1.axes.get_yaxis().set_ticklabels(['Winter','Spring','Summer','Fall'])\n",
    "    else:\n",
    "        frame1 = plt.gca()\n",
    "        frame1.axes.xaxis.set_ticklabels([])\n",
    "        frame1.axes.get_yaxis().set_ticks([1,4,7,10])\n",
    "        frame1.axes.get_yaxis().set_ticklabels(['Winter','Spring','Summer','Fall'])\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.95, 0.25, 0.03, 0.5])\n",
    "cbar = fig.colorbar(cp,cax=cbar_ax,orientation='vertical',ticks=[2,4,6,8,10,12,14,16,18,20])\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "# cbar.ax.set_xticklabels()\n",
    "cbar.set_label('Group',fontsize=14)\n",
    "\n",
    "if save_plots==True:\n",
    "    plt.savefig(sfpath+'GMM_hov_subgroup.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save all this data (both the original profiles as well as the PCA, GMM, and average profile data). That way you can work with the data offline later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find averages\n",
      "Save Data\n",
      "KeysView(<xarray.Dataset>\n",
      "Dimensions:         (n_comps: 21, pcs: 17, profs: 71272, t: 71272, z: 51)\n",
      "Dimensions without coordinates: n_comps, pcs, profs, t, z\n",
      "Data variables:\n",
      "    CHLA            (t, z) float64 1.456 1.456 1.456 ... 0.006506 0.004381\n",
      "    DOXY            (t, z) float64 343.3 343.4 343.5 343.7 ... 167.4 167.0 166.6\n",
      "    TEMP            (t, z) float64 2.734 2.736 2.734 2.714 ... 13.71 13.71 13.71\n",
      "    PSAL            (t, z) float64 33.85 33.86 33.86 33.86 ... 38.6 38.6 38.61\n",
      "    NITRATE         (t, z) float64 nan nan nan nan nan ... nan nan nan nan nan\n",
      "    CHLA_test       (t, z) float64 1.486 1.486 1.486 ... 0.03535 0.03651 0.03438\n",
      "    LATITUDE        (t) float64 -48.48 -48.48 -48.49 -48.48 ... 40.96 41.01 41.1\n",
      "    LONGITUDE       (t) float64 72.2 72.19 72.19 72.18 ... 5.936 5.901 5.922\n",
      "    JULD            (t) datetime64[ns] 2011-10-29T18:16:44.999980288 ... 2020...\n",
      "    PRES            (t, z) float64 0.0 5.043 10.09 15.13 ... 242.0 247.1 252.1\n",
      "    DEPTH           (t, z) float64 0.0 5.0 10.0 15.0 ... 235.0 240.0 245.0 250.0\n",
      "    gmm_means       (n_comps, pcs) float64 -4.776 2.747 ... -0.0006116\n",
      "    gmm_probs       (profs, n_comps) float64 8.833e-24 4.074e-07 ... 0.0\n",
      "    gmm_cov         (n_comps, pcs, pcs) float64 2.407 1.603 ... 0.01526\n",
      "    gmm_labels      (profs) int64 19 12 19 19 19 19 19 19 ... 16 16 16 1 1 1 14\n",
      "    gmm_new_groups  (n_comps) int64 5 0 10 3 7 18 13 2 ... 8 12 1 9 17 4 14 11\n",
      "    gmm_new_labels  (profs) float64 8.0 14.0 8.0 8.0 8.0 ... 15.0 15.0 15.0 19.0\n",
      "    pc_comps        (pcs, z) float64 0.2703 0.2725 0.2735 ... -0.07671 -0.1835\n",
      "    pc_transform    (profs, pcs) float64 8.214 6.686 ... 0.07502 -0.08234\n",
      "    pc_var          (pcs) float64 0.4525 0.2924 0.1317 ... 0.0008474 0.0007502\n",
      "    CHLA_ave        (n_comps, z) float64 0.02512 0.02463 ... 0.007504 0.006466\n",
      "    CHLA_std        (n_comps, z) float64 0.02272 0.0222 ... 0.02711 0.02276\n",
      "    DOXY_ave        (n_comps, z) float64 -2.065e+05 266.5 241.3 ... 42.82 42.5\n",
      "    DOXY_std        (n_comps, z) float64 1.371e+07 2.526e+03 ... 1.435e+04\n",
      "    TEMP_ave        (n_comps, z) float64 -10.86 24.71 24.62 ... 7.901 7.856\n",
      "    TEMP_std        (n_comps, z) float64 2.096e+03 3.503 3.505 ... 6.19 6.125\n",
      "    PSAL_ave        (n_comps, z) float64 66.6 36.36 36.36 ... 35.07 35.07 35.07\n",
      "    PSAL_std        (n_comps, z) float64 1.864e+03 1.999 1.993 ... 1.632 1.627\n",
      "    NITRATE_ave     (n_comps, z) float64 0.4415 0.4526 0.4503 ... 5.339 5.395\n",
      "    NITRATE_std     (n_comps, z) float64 36.6 36.46 36.34 ... 11.4 11.42 11.46)\n"
     ]
    }
   ],
   "source": [
    "print('Find averages')\n",
    "for var in ['DOXY','NITRATE','TEMP','PSAL']:\n",
    "    group_wave[var]=np.nan*np.ones((gmm_comps,51))\n",
    "    group_wstd[var]=np.nan*np.ones((gmm_comps,51))\n",
    "\n",
    "for var in ['DOXY','NITRATE','TEMP','PSAL']:\n",
    "    masked_data = np.ma.masked_array(data[var], np.isnan(data[var]))\n",
    "    #masked_data = np.ma.masked_array(masked_data, np.isinf(masked_data))\n",
    "    for m in range(gmm_comps):\n",
    "        d=[i for i,j in enumerate(labels) if j==m]\n",
    "        #weighted average\n",
    "        group_wave[var][m]=np.average(masked_data[d],0,weights=probs[d,m])\n",
    "        #weighted standard deviation\n",
    "        variance = np.average((masked_data[d]-group_wave[var][m])**2,0, weights=probs[d,m])\n",
    "        group_wstd[var][m]=variance**0.5\n",
    "\n",
    "print('Save Data')\n",
    "gmm_data={}\n",
    "gmm_data['gmm_means']={'dims':('n_comps','pcs'),'data':gmm.means_}\n",
    "gmm_data['gmm_probs']={'dims':('profs','n_comps'),'data':probs}\n",
    "gmm_data['gmm_cov']={'dims':('n_comps','pcs','pcs'),'data':gmm.covariances_}\n",
    "gmm_data['gmm_labels']={'dims':('profs'),'data':labels}\n",
    "gmm_data['gmm_new_groups']={'dims':('n_comps'),'data':new_groups}\n",
    "gmm_data['gmm_new_labels']={'dims':('profs'),'data':new_labels}\n",
    "gmm_data['pc_comps']={'dims':('pcs','z'),'data':pca.components_}\n",
    "gmm_data['pc_transform']={'dims':('profs','pcs'),'data':training}\n",
    "gmm_data['pc_var']={'dims':('pcs'),'data':pca.explained_variance_ratio_}\n",
    "\n",
    "for var in ['CHLA','DOXY','TEMP','PSAL','NITRATE']:\n",
    "    gmm_data[var+'_ave']={'dims':('n_comps','z'),'data':group_wave[var]}\n",
    "    gmm_data[var+'_std']={'dims':('n_comps','z'),'data':group_wstd[var]}\n",
    "    \n",
    "\n",
    "dict_data={}\n",
    "for v in ['CHLA','DOXY','TEMP','PSAL','NITRATE']:\n",
    "    dict_data[v]={'dims':('t','z'),'data':data[v]}\n",
    "    \n",
    "dict_data['CHLA_test']={'dims':('t','z'),'data':test_data}\n",
    "\n",
    "for v in ['LATITUDE','LONGITUDE','JULD']:\n",
    "    dict_data[v]={'dims':('t'),'data':data[v]}\n",
    "\n",
    "for v in ['PRES','DEPTH']:\n",
    "    dict_data[v]={'dims':('t','z'),'data':data[v]}\n",
    "\n",
    "#GMM data\n",
    "for var in gmm_data.keys():\n",
    "    dict_data[var]=gmm_data[var]\n",
    "\n",
    "ds=xr.Dataset.from_dict(dict_data)\n",
    "print(ds.keys())\n",
    "\n",
    "filename=sfpath+'gmm_results_%dpcs_%dclusters.nc' %(pcs,gmm_comps)\n",
    "\n",
    "ds.to_netcdf(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for playing around with this. Please let me know if you have any feedback, thoughts, or want to collaborate on future related projects!\n",
    "Rosalind Echols\n",
    "rechols@uw.edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
